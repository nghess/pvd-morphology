{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Tiff Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pvd_io import *\n",
    "from pvd_par import PVD\n",
    "from pvd_metrics import data_summary\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Check on slurm job\n",
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Located 53 tiff stacks\n"
     ]
    }
   ],
   "source": [
    "# Get all tiff stacks and their paths\n",
    "data_dir = 'pvd_data/'\n",
    "min_file_size = 1e8  # 100 MB\n",
    "datasets, sessions, files = scan_directories(data_dir, min_file_size)\n",
    "print(f\"Located {len(files)} tiff stacks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute pipeline\n",
    "for ii, file in enumerate(files):  # Slice like so to get a range of files[:1]\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Set path\n",
    "    data_path = 'pvd_data'\n",
    "    results_path = 'pvd_analysis'\n",
    "    dataset = datasets[ii]\n",
    "    session = sessions[ii]\n",
    "    output_path = f\"{results_path}/{dataset}/{session}/\"\n",
    "\n",
    "    # Capture and display output\n",
    "    output_capture = OutputCapture()\n",
    "    sys.stdout = output_capture\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing tiff stack {ii}/{len(files)}\")\n",
    "        print(f\"{datasets[ii]} {sessions[ii]}\")\n",
    "        pvd = PVD(data_path, dataset, session, file)\n",
    "        pvd.run_pipeline()\n",
    "\n",
    "        print(f\"Saving data to drive...\")\n",
    "        pvd.save_results(output_path, save_tiff=False, save_numpy=False, save_plotly=True, save_labeled_tiff=False)\n",
    "\n",
    "    finally:\n",
    "        sys.stdout = output_capture.original_stdout\n",
    "\n",
    "    # Save cell output\n",
    "    with open(f\"{output_path}output.txt\", 'w') as f:\n",
    "        f.write(str(output_capture.get_output()))\n",
    "\n",
    "    # Clear memory\n",
    "    del pvd\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create summary file recording MIP similarity between timepoints and segments matched\n",
    "data_summary(files, datasets, sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline.\n",
      "Data loaded. Shape: (4, 188, 2044, 2042): 4.51 seconds\n",
      "Data cropped. Shape: (4, 188, 2000, 2000): 8.81 seconds\n",
      "Preprocessing complete: 149.39 seconds\n",
      "Data skeletonized: 268.53 seconds\n",
      "Number of tips per timepoint: [99, 104, 105, 106]\n",
      "Number of knots per timepoint: [266, 281, 277, 271]\n",
      "Outer segments found. Number of outer segments per timepoint: [52, 52, 48, 55]: 5.00 seconds\n",
      "Matched 27 segments across all timepoints.\n",
      "Segments matched. Number of matched segments per timepoint: [27, 27, 27, 27]: 0.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/projects/serenolab/nhess/PVD/pvd_par.py:157: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  length, _ = quad(length_element, dist[0], dist[-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment length changes of time logged: 114.09 seconds\n",
      "Unmatched segments grouped: 9.38 seconds\n",
      "All timepoints labeled successfully\n",
      "Number of unique labels: 28\n",
      "Processed data labeled: 36.87 seconds\n",
      "Volume changes DataFrame generated: 44.03 seconds\n",
      "Pipeline complete. Total time: 640.78 seconds\n",
      "Saving data to drive...\n",
      "\n",
      "Quality Score: 0.7897\n",
      "Skeleton visualizations saved to pvd_analysis/DataSet01/exp240202_01_E_//visualizations\n",
      "Outer segment visualizations saved to pvd_analysis/DataSet01/exp240202_01_E_//visualizations\n",
      "Volume changes DataFrame saved to pvd_analysis/DataSet01/exp240202_01_E_//segment_change.csv\n"
     ]
    }
   ],
   "source": [
    "# Set path to original test file\n",
    "data_path = 'pvd_data'\n",
    "results_path = 'pvd_analysis'\n",
    "dataset = 'DataSet01'\n",
    "session = 'exp240202_01_E_'\n",
    "file = 'exp240202_01_E.tif'\n",
    "tiff_stack_path = f\"{data_path}/{dataset}/{session}_/{file}\"\n",
    "output_path = f\"{results_path}/{dataset}/{session}/\"\n",
    "\n",
    "# Process stack\n",
    "pvd = PVD(data_path, dataset, session, file)\n",
    "pvd.run_pipeline()\n",
    "\n",
    "# Save relevant numpy arrays\n",
    "print(f\"Saving data to drive...\")\n",
    "pvd.save_results(output_path, save_tiff=False, save_numpy=False, save_plotly=True, save_labeled_tiff=False)\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline.\n",
      "Data loaded. Shape: (4, 87, 2044, 2042): 3.11 seconds\n",
      "Data cropped. Shape: (4, 87, 2000, 2000): 4.12 seconds\n",
      "Preprocessing complete: 68.18 seconds\n",
      "Data skeletonized: 125.85 seconds\n",
      "Number of tips per timepoint: [145, 153, 161, 137]\n",
      "Number of knots per timepoint: [336, 352, 345, 284]\n",
      "Outer segments found. Number of outer segments per timepoint: [47, 43, 45, 47]: 2.64 seconds\n",
      "Matched 20 segments across all timepoints.\n",
      "Segments matched. Number of matched segments per timepoint: [20, 20, 20, 20]: 0.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/projects/serenolab/nhess/PVD/pvd_par.py:157: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  length, _ = quad(length_element, dist[0], dist[-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment length changes over timepoints logged: 63.51 seconds\n",
      "Unmatched segments grouped: 4.37 seconds\n",
      "All timepoints labeled successfully\n",
      "Number of unique labels: 21\n",
      "Processed data labeled: 17.88 seconds\n",
      "Volume changes DataFrame generated: 19.66 seconds\n",
      "Pipeline complete. Total time: 309.45 seconds\n",
      "Saving data to drive...\n",
      "\n",
      "Quality Score: 0.6833\n",
      "Skeleton visualizations saved to pvd_analysis/DataSet00/exp240129_01_03_//visualizations\n",
      "Outer segment visualizations saved to pvd_analysis/DataSet00/exp240129_01_03_//visualizations\n",
      "Volume changes DataFrame saved to pvd_analysis/DataSet00/exp240129_01_03_//segment_change.csv\n"
     ]
    }
   ],
   "source": [
    "# Set path to a *bad* test file\n",
    "data_path = 'pvd_data'\n",
    "results_path = 'pvd_analysis'\n",
    "dataset = 'DataSet00'\n",
    "session = 'exp240129_01_03_'\n",
    "file = 'exp240129_01_03.tif'\n",
    "tiff_stack_path = f\"{data_path}/{dataset}/{session}_/{file}\"\n",
    "output_path = f\"{results_path}/{dataset}/{session}/\"\n",
    "\n",
    "# Process stack\n",
    "pvd = PVD(data_path, dataset, session, file)\n",
    "pvd.run_pipeline()\n",
    "\n",
    "# Save relevant numpy arrays\n",
    "print(f\"Saving data to drive...\")\n",
    "pvd.save_results(output_path, save_tiff=False, save_numpy=False, save_plotly=True, save_labeled_tiff=False)\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  -6, -10,  -7],\n",
       "       [  6,   0,  -4,  -1],\n",
       "       [ 10,   4,   0,   3],\n",
       "       [  7,   1,  -3,   0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvd.length_change_voxels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,  -6.71317871, -15.31028191,  -9.95582884],\n",
       "       [  6.71317871,   0.        ,  -8.59710321,  -3.24265013],\n",
       "       [ 15.31028191,   8.59710321,   0.        ,   5.35445308],\n",
       "       [  9.95582884,   3.24265013,  -5.35445308,   0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvd.length_change_spline[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(49), np.int64(1598), np.int64(656))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pvd.matched_segments[0][9][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matched Segment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a line to extract the quality score <-- add this to pvd_metrics.py\n",
    "import pandas as pd\n",
    "\n",
    "# Create a file with a tally of matched segments per tiff stack\n",
    "\n",
    "def data_summary(files, datasets, sessions, summary_file=\"data_summary_100324.txt\"):\n",
    "    # Summary Report\n",
    "    for ii, file in enumerate(files):\n",
    "        results_path = 'pvd_analysis'\n",
    "        dataset = datasets[ii]\n",
    "        session = sessions[ii]\n",
    "        output_path = f\"{results_path}/{dataset}/{session}/\"\n",
    "\n",
    "        segment_csv = pd.read_csv(f\"{output_path}segment_change.csv\")\n",
    "        quality_csv = pd.read_csv(f\"{output_path}mip_cosine_similarity.csv\")\n",
    "        csv_length = segment_csv.shape[1]-3  # Subtract 3 for index and core segment\n",
    "        quality_score = quality_csv.loc[5,'t1']  # Get avg. cosine similarity score from csv\n",
    "\n",
    "        with open(summary_file, 'a') as file:\n",
    "            file.write(f\"{dataset} - {session} -- Segments: {csv_length}  Quality: {float(quality_score):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary(files, datasets, sessions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
