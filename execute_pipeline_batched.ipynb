{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Tiff Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pvd_par import PVD\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get files and paths\n",
    "def scan_directories(data_directory, min_size_bytes):\n",
    "    dataset_list = []\n",
    "    session_list = []\n",
    "    file_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(data_directory):\n",
    "        # Check if the current path contains \"exclude\" or \"movement\"\n",
    "        if \"exclude\" in root.lower() or \"movement\" in root.lower() or \"missing\" in root.lower():\n",
    "            continue  # Skip this directory\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith('.tif'):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Check file size\n",
    "                if os.path.getsize(file_path) < min_size_bytes:\n",
    "                    continue  # Skip this file if it's too small\n",
    "\n",
    "                path_parts = os.path.normpath(root).split(os.sep)\n",
    "                if len(path_parts) >= 3:\n",
    "                    dataset = path_parts[-2]\n",
    "                    session = path_parts[-1]\n",
    "                    dataset_list.append(dataset)\n",
    "                    session_list.append(session)\n",
    "                    file_list.append(file)\n",
    "\n",
    "    return dataset_list, session_list, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Located 53 tiff stacks\n"
     ]
    }
   ],
   "source": [
    "# Get all tiff stacks and their paths\n",
    "data_dir = 'S:/pvd_data/'\n",
    "min_file_size = 1e8  # 100 MB\n",
    "datasets, sessions, files = scan_directories(data_dir, min_file_size)\n",
    "print(f\"Located {len(files)} tiff stacks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execute pipeline\n",
    "# for ii, file in enumerate(files):  # Slice like so to get a range of files[:1]\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "#     # Set path\n",
    "#     data_path = 'pvd_data'\n",
    "#     results_path = 'pvd_analysis'\n",
    "#     dataset = datasets[ii]\n",
    "#     session = sessions[ii]\n",
    "#     output_path = f\"{results_path}/{dataset}/{session}/\"\n",
    "\n",
    "#     # Process stack\n",
    "#     print(f\"Processing tiff stack {ii}/{len(files)}\")\n",
    "#     print(f\"{datasets[ii]} {sessions[ii]}\")\n",
    "#     pvd = PVD(data_path, dataset, session, file)\n",
    "#     pvd.run_pipeline()\n",
    "\n",
    "#     # Save relevant numpy arrays\n",
    "#     print(f\"Saving data to drive...\")\n",
    "#     pvd.save_results(output_path, save_tiff=False, save_numpy=False, save_plotly=True, save_labeled_tiff=False)\n",
    "\n",
    "#     del pvd\n",
    "\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set path to zero-match file\n",
    "# data_path = 'pvd_data'\n",
    "# results_path = 'pvd_analysis'\n",
    "# dataset = 'DataSet00'\n",
    "# session = 'exp240104_00_01_'\n",
    "# file = 'exp240104_00_01.tif'\n",
    "# tiff_stack_path = f\"{data_path}/{dataset}/{session}_/{file}\"\n",
    "# output_path = f\"{results_path}/{dataset}/{session}/\"\n",
    "\n",
    "# # Process stack\n",
    "# pvd = PVD(data_path, dataset, session, file)\n",
    "# pvd.run_pipeline()\n",
    "\n",
    "# # Save relevant numpy arrays\n",
    "# print(f\"Saving data to drive...\")\n",
    "# pvd.save_results(output_path, save_tiff=False, save_numpy=False, save_plotly=True, save_labeled_tiff=False)\n",
    "\n",
    "# time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline.\n",
      "Data loaded. Shape: (4, 188, 2044, 2042): 49.12 seconds\n",
      "Data cropped. Shape: (4, 188, 2000, 2000): 6.67 seconds\n",
      "Preprocessing complete: 239.66 seconds\n",
      "Data skeletonized: 328.02 seconds\n",
      "Number of tips per timepoint: [117, 112, 117, 122]\n",
      "Number of knots per timepoint: [342, 345, 335, 340]\n",
      "Outer segments found. Number of outer segments per timepoint: [34, 32, 32, 35]: 5.70 seconds\n",
      "Matched 15 segments across all timepoints.\n",
      "Segments matched. Number of matched segments per timepoint: [15, 15, 15, 15]: 7.81 seconds\n",
      "Unmatched segments grouped: 7.77 seconds\n",
      "All timepoints labeled successfully\n",
      "Number of unique labels: 16\n",
      "Processed data labeled: 48.12 seconds\n",
      "Volume changes DataFrame generated: 48.73 seconds\n",
      "Pipeline complete. Total time: 741.61 seconds\n",
      "Saving data to drive...\n",
      "Skeleton visualizations saved to S:/pvd_analysis/DataSet01/exp240202_01_E_//visualizations\n",
      "Outer segment visualizations saved to S:/pvd_analysis/DataSet01/exp240202_01_E_//visualizations\n",
      "Volume changes DataFrame saved to S:/pvd_analysis/DataSet01/exp240202_01_E_//segment_change.csv\n"
     ]
    }
   ],
   "source": [
    "# Set path to original test file\n",
    "data_path = 'pvd_data'\n",
    "results_path = 'pvd_analysis'\n",
    "dataset = 'DataSet01'\n",
    "session = 'exp240202_01_E_'\n",
    "file = 'exp240202_01_E.tif'\n",
    "tiff_stack_path = f\"{data_path}/{dataset}/{session}_/{file}\"\n",
    "output_path = f\"{results_path}/{dataset}/{session}/\"\n",
    "\n",
    "# Process stack\n",
    "pvd = PVD(data_path, dataset, session, file)\n",
    "pvd.run_pipeline()\n",
    "\n",
    "# Save relevant numpy arrays\n",
    "print(f\"Saving data to drive...\")\n",
    "pvd.save_results(output_path, save_tiff=False, save_numpy=False, save_plotly=True, save_labeled_tiff=False)\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matched Segment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# summary_file = \"summary.txt\"\n",
    "\n",
    "# # Summary Report\n",
    "# for ii, file in enumerate(files):\n",
    "#     results_path = 'pvd_analysis'\n",
    "#     dataset = datasets[ii]\n",
    "#     session = sessions[ii]\n",
    "#     output_path = f\"{results_path}/{dataset}/{session}/\"\n",
    "\n",
    "#     csv = pd.read_csv(f\"{output_path}segment_change.csv\")\n",
    "#     csv_length = csv.shape[1]-3  # Subtract 3 for index and core segment\n",
    "\n",
    "#     with open(summary_file, 'a') as file:\n",
    "#         file.write(f\"{dataset} - {session}: {csv_length}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
